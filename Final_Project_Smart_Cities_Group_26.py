# -*- coding: utf-8 -*-
"""Final_Project_smart_cities.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r01StTAc-YxaGs4-dKLXSHN0lptwYsuT

## Importing Libraries
"""

# Essential libraries
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 

# Sklearn libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.ensemble import VotingClassifier

# Tensorflow libraries
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from tensorflow.keras import layers
from keras.layers import InputLayer, Dense, Dropout,BatchNormalization
from keras.layers.advanced_activations import LeakyReLU
from tensorflow.keras.optimizers import Adam

"""## Importing the dateset"""

data = pd.read_csv('/content/MCSDatasetNEXTCONLab.csv')
data.head()

# Splitting the dataset into inputs and outputs
x = data.iloc[:,:-1].values
y = data.iloc[:,-1].values
x

y

"""##Splitting dataset into train and test sets"""

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2, random_state = 0)
print(x_train)
print(x_test),
print(y_train)
print(y_test)

"""# Scaling"""

sc = MinMaxScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

"""## Modeling"""

# Building function contains all classic machine learning model
def model(x_train,y_train,x_test):
  #Random Forest
  RF = RandomForestClassifier(n_estimators=100,random_state =0)
  RF.fit(x_train,y_train)
  y_pred_RF = RF.predict(x_test)

  # Naive bayes
  NB = GaussianNB()
  NB.fit(x_train,y_train)
  y_pred_NB = NB.predict(x_test)

  #Adaboost
  Adaboost = AdaBoostClassifier(n_estimators = 100, random_state = 0)
  Adaboost.fit(x_train,y_train)
  y_pred_Adaboost = Adaboost.predict(x_test)

  return y_pred_RF, y_pred_NB, y_pred_Adaboost,RF,NB,Adaboost

y_pred_RF, y_pred_NB,y_pred_Adaboost,RF,NB,Adaboost = model(x_train,y_train,x_test)

"""## Evaluation"""

# Building function contain all evaluation techniques we used in this project
def accuracy (y_test,y_pred):
  cr = classification_report(y_test,y_pred)
  acc = accuracy_score(y_test,y_pred)
  print(cr)
  return acc

"""1. Random Forest"""

accuracy(y_test,y_pred_RF)

"""2. Naive Bayes"""

accuracy(y_test,y_pred_NB)

"""3. Adaboost """

accuracy(y_test,y_pred_Adaboost)

"""## Final decision method 1"""

all_models = {'RF':y_pred_RF,'NB':y_pred_NB,'Adaboost':y_pred_Adaboost}
df = pd.DataFrame(all_models)
Aggregator = df.sum(axis=1)
final_decision = []
for i in Aggregator:
  if i >= 2:
    final_decision.append(1)
  else:
    final_decision.append(0)
final_decision

# def voting(x_train,y_train,x_test,RF,NB,Adaboost):
#   model(x_train,y_train,x_test)
#   voting = VotingClassifier(estimators=[('RF',RF),('NB',NB),('Adaboost',Adaboost)], voting = 'hard')
#   voting.fit(x_train,y_train)
#   final_decision = voting.predict(x_test)
#   return final_decision

# final_decision = voting(x_train,y_train,x_test,RF,NB,Adaboost)
# final_decision

"""## Final decision method 2"""

y_RF = RF.predict(x_train)
y_NB = NB.predict(x_train)
y_Adaboost = Adaboost.predict(x_train)
X = accuracy(y_train,y_RF)
Y = accuracy(y_train,y_Adaboost)
Z = accuracy(y_train,y_NB)
W_RF = X/(X+Y+Z)
W_Adaboost = Y/(X+Y+Z)
W_NB = Z/(X+Y+Z)
Aggregated_output = y_pred_RF * W_RF + y_pred_NB * W_NB + y_pred_Adaboost * W_Adaboost
print(Aggregated_output)

final_decision_2 = []
for i in Aggregated_output:
  if i > 0.5 :
    final_decision_2.append(1)
  else:
    final_decision_2.append(0)

final_decision_2

"""## Comparision"""

Ensemble_vote = accuracy(y_test,final_decision)
Ensemble_wieghted = accuracy(y_test,final_decision_2)
acc_RF = accuracy(y_test,y_pred_RF)
acc_NB = accuracy(y_test,y_pred_NB)
acc_Adaboost = accuracy(y_test,y_pred_Adaboost)

"""## Visualization"""

import seaborn as sns
from IPython.core.pylabtools import figsize
figsize(10,7)
sns.barplot(x=['RF','Adaboost','NB','Ensemble vote','Ensemble wieghted'], y = [acc_RF,acc_Adaboost,acc_NB,Ensemble_vote,Ensemble_wieghted],palette='dark:salmon_r')
plt.ylim(0.8,1)

"""# GAN

## Constant variables
"""

batch_size = 64
num_channels = 12
num_classes = 2
latent_dim = 128

"""## Prepare the training data

"""

y_train_new = keras.utils.to_categorical(y_train, 2)
dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train_new))
dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)

print(f"Shape of training images: {x_train.shape}")
print(f"Shape of training labels: {y_train_new.shape}")

generator_in_channels = latent_dim + num_classes
discriminator_in_channels = num_channels + num_classes
print(generator_in_channels, discriminator_in_channels)

"""## Building Generator and Discriminator models"""

# Create the discriminator.
discriminator = Sequential(
    [
        Dense(512,input_dim = discriminator_in_channels),
        LeakyReLU(alpha=0.2),
        Dense(512),
        LeakyReLU(alpha=0.2),
        Dropout(0.4),
        Dense(512),
        LeakyReLU(alpha=0.2),
        Dropout(0.4),
        Dense(1, activation='sigmoid')
    ],
    name="discriminator"
)

# Create the generator.
generator = Sequential(
    [
        Dense(256,input_dim = generator_in_channels),
        LeakyReLU(alpha=0.2),
        BatchNormalization(momentum=0.8),
        Dense(512),
        LeakyReLU(alpha=0.2),
        BatchNormalization(momentum=0.8),
        Dense(1024),
        LeakyReLU(alpha=0.2),
        BatchNormalization(momentum=0.8),
        Dense(12)],
    name="generator"
)

"""## Conditional GAN"""

class ConditionalGAN(keras.Model):
    def __init__(self, discriminator, generator, latent_dim):
        super(ConditionalGAN, self).__init__()
        self.discriminator = discriminator
        self.generator = generator
        self.latent_dim = latent_dim
        self.gen_loss_tracker = keras.metrics.Mean(name="generator_loss")
        self.disc_loss_tracker = keras.metrics.Mean(name="discriminator_loss")

    @property
    def metrics(self):
        return [self.gen_loss_tracker, self.disc_loss_tracker]

    def compile(self, d_optimizer, g_optimizer, loss_fn):
        super(ConditionalGAN, self).compile()
        self.d_optimizer = d_optimizer
        self.g_optimizer = g_optimizer
        self.loss_fn = loss_fn

    def train_step(self, data):
        # Unpack the data.
        real_tasks, one_hot_labels = data

        # Add dummy dimensions to the labels so that they can be concatenated with the tasks. This is for the discriminator.
        task_one_hot_labels = one_hot_labels[:, :, None, None]
        task_one_hot_labels = tf.reshape(task_one_hot_labels, (-1, num_classes))


        # Sample random points in the latent space and concatenate the labels. This is for the generator.
        batch_size = tf.shape(real_tasks)[0]
        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))
        random_vector_labels = tf.concat([random_latent_vectors, one_hot_labels], axis=1)

        # Decode the noise (guided by labels) to fake tasks.
        generated_tasks = self.generator(random_vector_labels)

        # Combine them with real tasks. Note that we are concatenating the labels with these tasks here.
        real_tasks = tf.cast(real_tasks, tf.float32)
        fake_task_and_labels = tf.concat([generated_tasks, task_one_hot_labels], -1)
        real_task_and_labels = tf.concat([real_tasks, task_one_hot_labels], -1)
        combined_tasks = tf.concat([fake_task_and_labels, real_task_and_labels], axis=0)

        # Assemble labels discriminating real from fake tasks.
        labels = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)

        # Train the discriminator.
        with tf.GradientTape() as tape:
            predictions = self.discriminator(combined_tasks)
            d_loss = self.loss_fn(labels, predictions)
        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)
        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))

        # Sample random points in the latent space.
        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))
        random_vector_labels = tf.concat([random_latent_vectors, one_hot_labels], axis=1)

        # Assemble labels that say "all real images".
        misleading_labels = tf.zeros((batch_size, 1))

        # Train the generator (note that we should *not* update the weights of the discriminator)!
        with tf.GradientTape() as tape:
            fake_tasks = self.generator(random_vector_labels)
            fake_task_and_labels = tf.concat([fake_tasks, task_one_hot_labels], -1)
            predictions = self.discriminator(fake_task_and_labels)
            g_loss = self.loss_fn(misleading_labels, predictions)
        grads = tape.gradient(g_loss, self.generator.trainable_weights)
        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))

        # Monitor loss.
        self.gen_loss_tracker.update_state(g_loss)
        self.disc_loss_tracker.update_state(d_loss)
        return {
            "g_loss": self.gen_loss_tracker.result(),
            "d_loss": self.disc_loss_tracker.result(),
        }

cond_gan = ConditionalGAN(
    discriminator=discriminator, generator=generator, latent_dim=latent_dim
)
cond_gan.compile(
    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),
    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),
    loss_fn=keras.losses.BinaryCrossentropy(),
)

cond_gan.fit(dataset, epochs=20)

"""# Generate fake task  via generator network"""

# We first extract the trained generator from our Conditional GAN.
trained_gen = cond_gan.generator

# Choose the number of intermediate tasks that would be generated
num_interpolation = 2000  # @param {type:"integer"}

# Sample noise for the interpolation.
interpolation_noise = tf.random.normal(shape=(1, latent_dim))
interpolation_noise = tf.repeat(interpolation_noise, repeats=num_interpolation)
interpolation_noise = tf.reshape(interpolation_noise, (num_interpolation, latent_dim))


def interpolate_class(class1):
    # Convert the start label to one-hot encoded vectors.
    first_label = keras.utils.to_categorical([class1]*num_interpolation, num_classes)
    
    # Combine the noise and the labels and run inference with the generator.
    noise_and_labels = tf.concat([interpolation_noise, first_label], 1)
    fake = trained_gen.predict(noise_and_labels)
    return fake


class1 = 1 

fake_tasks = interpolate_class(class1)

"""# Mix fake task with the original test dataset




"""

x_test_new = np.concatenate((x_test,fake_tasks),axis = 0)
x_test_new

fake_task_label = np.zeros(num_interpolation)
y_test_new = np.concatenate((y_test,fake_task_label),axis = 0)
y_test_new

"""# Train the machine learning models with new test set"""

y_pred_RF_new, y_pred_NB_new, y_pred_Adaboost_new,RF_new,NB_new,Adaboost_new =  model(x_train,y_train,x_test_new)

acc_RF_new = accuracy(y_test_new,y_pred_RF_new)

acc_Adaboost_new = accuracy(y_test_new,y_pred_Adaboost_new)

acc_NB_new = accuracy(y_test_new,y_pred_NB_new)

figsize(10,7)
sns.barplot(x=['RF','Adaboost'], y = [acc_RF_new,acc_Adaboost_new],palette='dark:salmon_r')
plt.title("Accuracy under mixed test dataset")

"""# Cascade framework"""

y = keras.utils.to_categorical(y_test_new, 2)
data = tf.concat([x_test_new,y],axis=1)
new_prediction = cond_gan.discriminator.predict(data)
new_prediction

new_prediction = np.round(new_prediction).astype(int)
new_prediction

idx = np.where(new_prediction == 1)[0]
x_real_cascade = x_test_new[idx]
y_real_discriminator = np.ones(x_real_cascade.shape[0])

y_pred_RF_cascade, y_pred_NB_cascade, y_pred_Adaboost_cascade,RF_cascade,NB_cascade,Adaboost_cascade =  model(x_train,y_train,x_real_cascade)

acc_RF_cascade = accuracy(y_real_discriminator,y_pred_RF_cascade)

acc_Adaboost_cascade = accuracy(y_real_discriminator,y_pred_Adaboost_cascade)

figsize(10,7)
sns.barplot(x=['RF-based Cascade','Adaboost-based Cascade'], y = [acc_RF_cascade,acc_Adaboost_cascade],palette='dark:salmon_r')
plt.title("Accuracy under mixed test dataset")